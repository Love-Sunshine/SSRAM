{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T08:59:12.454568Z",
     "start_time": "2022-03-05T08:59:09.698378Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-16T08:26:46.977359Z",
     "iopub.status.busy": "2021-07-16T08:26:46.976760Z",
     "iopub.status.idle": "2021-07-16T08:26:49.706903Z",
     "shell.execute_reply": "2021-07-16T08:26:49.705463Z",
     "shell.execute_reply.started": "2021-07-16T08:26:46.977278Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "initial_directory = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchsummary\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "from network.CG_CN import CG_CN\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # 以下面设置的第一个卡为主卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,0\"  # 物理卡号\n",
    "\n",
    "config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T08:59:14.057994Z",
     "start_time": "2022-03-05T08:59:12.457093Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "config.update(\n",
    "    {\n",
    "        'size': 180,\n",
    "        'in_channels': 1,\n",
    "        'encoder_channels': [32, 64, 128, 256, 512],\n",
    "        'decoder_channels': [512, 256, 128, 64, 32],\n",
    "        'out_channels': 2,\n",
    "    }\n",
    ")\n",
    "config.update({\"net_name\": \"CG_CN\"})\n",
    "net = CG_CN(config)\n",
    "print(net(torch.randn(8, 1, 180, 180)).shape)\n",
    "print(next(net.parameters()).device)\n",
    "config.update({\"parameters\": sum(param.numel() for param in net.parameters())})\n",
    "print(config[\"parameters\"])\n",
    "config.update({\"flops\": FlopCountAnalysis(net, (torch.randn(8, 1, 180, 180),)).total()})\n",
    "print(config[\"flops\"])\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T08:59:14.076333Z",
     "start_time": "2022-03-05T08:59:14.060873Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class My_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        assert data.shape[1] == 3, \"The data does not meet the requirements.\"\n",
    "        self.data = self.get_data(data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        origin, segmentation, label = self.data[index]\n",
    "        t=origin.reshape(origin.shape[0],-1)\n",
    "        t=(t-t.mean(axis=1,keepdim=True))/torch.max(t.std(axis=1,keepdim=True),1.0/torch.tensor(t.shape[1]*1.0).sqrt())\n",
    "        origin = t.reshape(origin.shape)\n",
    "        return origin.float(), segmentation.long(), int(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def get_data(self, data):\n",
    "        total = []\n",
    "        transformer = transforms.Compose([transforms.ToTensor()])\n",
    "        for i in tnrange(data.shape[0], dynamic_ncols=True, desc=\"get_data\"):\n",
    "            assert (os.path.exists(data[i][0]) and os.path.isfile((data[i][0])) and os.path.exists(data[i][1]) and os.path.isfile((data[i][1])))\n",
    "            origin = np.uint16(np.load(data[i][0]))\n",
    "            segmentation = np.uint16(np.load(data[i][1]))\n",
    "            assert (len(origin.shape) == 3 and len(segmentation.shape) == 3 and origin.shape == segmentation.shape)\n",
    "            for j in range(origin.shape[2]):\n",
    "                if len(np.unique(segmentation[:, :, j])) > 1:\n",
    "                    total.append([transformer(np.float32(origin[:,:,j])),transformer(np.float32(segmentation[:,:,j])/65535.0),int(data[i][2])])\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T08:59:14.116254Z",
     "start_time": "2022-03-05T08:59:14.078477Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation(data_iterator, net, loss_function, device):\n",
    "    net.eval()\n",
    "    net = net.to(device)\n",
    "    loss, number = 0.0, 0\n",
    "    TP, TN, FP, FN = 0.0, 0.0, 0.0, 0.0\n",
    "    y_hat_list,y_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, _, y in tqdm(data_iterator, dynamic_ncols=True, leave=False, desc=\"test\"):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            loss += loss_function(y_hat, y).float().cpu().item() * y.shape[0]\n",
    "            y_hat = y_hat.detach().cpu().float()\n",
    "            y_hat = nn.Softmax(dim=1)(y_hat)\n",
    "            y_hat_list.append(y_hat[:,1])\n",
    "            y_list.append(y.cpu().int())\n",
    "            y_hat = y_hat.argmax(dim=1).int().float()\n",
    "            TP += (((y_hat.int() == 1).int() + (y.cpu().int() == 1).int()) == 2).int().float().sum().item()\n",
    "            FP += (((y_hat.int() == 1).int() + (y.cpu().int() == 0).int()) == 2).int().float().sum().item()\n",
    "            FN += (((y_hat.int() == 0).int() + (y.cpu().int() == 1).int()) == 2).int().float().sum().item()\n",
    "            TN += (((y_hat.int() == 0).int() + (y.cpu().int() == 0).int()) == 2).int().float().sum().item()\n",
    "            number += y.shape[0]\n",
    "        assert len(y_hat_list) == len(y_list)\n",
    "        assert len(torch.cat(y_hat_list,dim=0)) == len(torch.cat(y_list,dim=0))\n",
    "        fpr, tpr, _ = metrics.roc_curve(torch.cat(y_list,dim=0), torch.cat(y_hat_list,dim=0),pos_label=1)\n",
    "        AUC = metrics.auc(fpr, tpr)\n",
    "        Acc = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0.0#Accuracy\n",
    "        SE = TP / (TP + FN) if TP + FN > 0 else 0.0# Sensitivity == Recall\n",
    "        SP = TN / (TN + FP) if TN + FP > 0 else 0.0#Specificity\n",
    "        PC = TP / (TP + FP) if TP + FP > 0 else 0.0#Precision\n",
    "        F1 = 2 * TP / (2 * TP + FN + FP) if 2 * TP + FN + FP > 0 else 0.0#F1 == DC\n",
    "        print('[Validation] Loss: %.4f, Acc: %.4f, SE: %.4f, SP: %.4f, PC: %.4f, F1: %.4f, AUC: %.4f' % (loss / number, Acc, SE, SP, PC, F1, AUC))\n",
    "    net.train()\n",
    "    return {'loss': loss / number,'Acc': Acc,'SE': SE,'SP': SP,'PC': PC,'F1': F1,'AUC': AUC}\n",
    "\n",
    "\n",
    "def train(net,train_iterator,test_iterator,loss_function,number_epochs,number_epochs_decay,optimizer,learning_rate,device,model_save_path):\n",
    "    net.train()\n",
    "    net = net.to(device)\n",
    "    print(\"training on\", device)\n",
    "    temporary_dictionary = {\n",
    "        \"train_loss\": [],\"train_Acc\": [],\"train_SE\": [],\"train_SP\": [],\"train_PC\": [],\"train_F1\": [],\"train_AUC\": [],\n",
    "        \"test_loss\": [],\"test_Acc\": [],\"test_SE\": [],\"test_SP\": [],\"test_PC\": [],\"test_F1\": [],\"test_AUC\": [],\n",
    "    }\n",
    "    for epoch in tnrange(1, number_epochs + 1, dynamic_ncols=True, desc=\"epoch\"):\n",
    "        assert net.training == True\n",
    "        train_loss, number, start_time = 0.0, 0, time.time()\n",
    "        TP, TN, FP, FN = 0.0, 0.0, 0.0, 0.0\n",
    "        y_hat_list,y_list = [], []\n",
    "        for X, _, y in tqdm(train_iterator, dynamic_ncols=True, leave=False, desc=\"train\"):\n",
    "            assert net.training == True\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            loss = loss_function(y_hat, y).float()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.float().cpu().item() * y.shape[0]\n",
    "            y_hat = y_hat.detach().cpu().float()\n",
    "            y_hat = nn.Softmax(dim=1)(y_hat)\n",
    "            y_hat_list.append(y_hat[:,1])\n",
    "            y_list.append(y.cpu().int())\n",
    "            y_hat = y_hat.argmax(dim=1).int().float()\n",
    "            TP += (((y_hat.int() == 1).int() + (y.cpu().int() == 1).int()) == 2).int().float().sum().item()\n",
    "            FP += (((y_hat.int() == 1).int() + (y.cpu().int() == 0).int()) == 2).int().float().sum().item()\n",
    "            FN += (((y_hat.int() == 0).int() + (y.cpu().int() == 1).int()) == 2).int().float().sum().item()\n",
    "            TN += (((y_hat.int() == 0).int() + (y.cpu().int() == 0).int()) == 2).int().float().sum().item()\n",
    "            number += y.shape[0]\n",
    "        assert len(y_hat_list) == len(y_list)\n",
    "        assert len(torch.cat(y_hat_list,dim=0)) == len(torch.cat(y_list,dim=0))\n",
    "        fpr, tpr, _ = metrics.roc_curve(torch.cat(y_list,dim=0), torch.cat(y_hat_list,dim=0),pos_label=1)\n",
    "        AUC = metrics.auc(fpr, tpr)\n",
    "        Acc = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0.0#Accuracy\n",
    "        SE = TP / (TP + FN) if TP + FN > 0 else 0.0# Sensitivity == Recall\n",
    "        SP = TN / (TN + FP) if TN + FP > 0 else 0.0#Specificity\n",
    "        PC = TP / (TP + FP) if TP + FP > 0 else 0.0#Precision\n",
    "        F1 = 2 * TP / (2 * TP + FN + FP) if 2 * TP + FN + FP > 0 else 0.0#F1 == DC\n",
    "        print('Epoch [%d/%d]' % (epoch, number_epochs))\n",
    "        print('[Training] Loss: %.4f, Acc: %.4f, SE: %.4f, SP: %.4f, PC: %.4f, F1: %.4f, AUC: %.4f' % (train_loss / number, Acc, SE, SP, PC, F1, AUC))\n",
    "        if epoch > (number_epochs - number_epochs_decay):\n",
    "            learning_rate *= (1 - epoch / number_epochs) ** 0.9\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "            print('Decay learning rate to lr: {}.'.format(learning_rate))\n",
    "        test_dictionary = evaluation(test_iterator, net, loss_function, device)\n",
    "\n",
    "        temporary_dictionary[\"train_loss\"].append(train_loss / number)\n",
    "        temporary_dictionary[\"train_Acc\"].append(Acc)\n",
    "        temporary_dictionary[\"train_SE\"].append(SE)\n",
    "        temporary_dictionary[\"train_SP\"].append(SP)\n",
    "        temporary_dictionary[\"train_PC\"].append(PC)\n",
    "        temporary_dictionary[\"train_F1\"].append(F1)\n",
    "        temporary_dictionary[\"train_AUC\"].append(AUC)\n",
    "\n",
    "        temporary_dictionary[\"test_loss\"].append(test_dictionary['loss'])\n",
    "        temporary_dictionary[\"test_Acc\"].append(test_dictionary['Acc'])\n",
    "        temporary_dictionary[\"test_SE\"].append(test_dictionary['SE'])\n",
    "        temporary_dictionary[\"test_SP\"].append(test_dictionary['SP'])\n",
    "        temporary_dictionary[\"test_PC\"].append(test_dictionary['PC'])\n",
    "        temporary_dictionary[\"test_F1\"].append(test_dictionary['F1'])\n",
    "        temporary_dictionary[\"test_AUC\"].append(test_dictionary['AUC'])\n",
    "        \n",
    "        torch.save(net, os.path.join(model_save_path, str(epoch) + \".pth\"))\n",
    "        print('Time %.1f sec' % (time.time() - start_time))\n",
    "    return temporary_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T08:59:32.602805Z",
     "start_time": "2022-03-05T08:59:14.118299Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_file_path = os.path.abspath(\"../data/data_192_save_as_resampled_qu/npy/npy_data_patients_xyz.xlsx\")\n",
    "config.update(\n",
    "    {\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"number_epochs\": 20,\n",
    "        \"number_epochs_decay\": 3,\n",
    "        \"test_size\": 0.1,\n",
    "    }\n",
    ")\n",
    "data = pd.read_excel(data_file_path)\n",
    "train_data, test_data = train_test_split(\n",
    "    data.values,\n",
    "    test_size=config[\"test_size\"],\n",
    "    random_state=42,\n",
    "    stratify=data.values[:, 2:],\n",
    ")\n",
    "train_dataset = My_Dataset(train_data)\n",
    "test_dataset = My_Dataset(test_data)\n",
    "train_iterator = torch.utils.data.DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "test_iterator = torch.utils.data.DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=config[\"learning_rate\"])\n",
    "model_save_path = os.path.abspath(\"../model/\" + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "pd.DataFrame(train_data, columns=data.columns).to_excel(os.path.join(model_save_path, 'train_data.xlsx'), sheet_name=\"train_data\", index=False)\n",
    "pd.DataFrame(test_data, columns=data.columns).to_excel(os.path.join(model_save_path, 'test_data.xlsx'), sheet_name=\"test_data\", index=False)\n",
    "pd.DataFrame.from_dict(config, orient='index').to_excel(os.path.join(model_save_path, 'config.xlsx'), sheet_name=\"config\")\n",
    "xlsx_path = os.path.join(model_save_path, config[\"net_name\"] + \".xlsx\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:44:26.626403Z",
     "start_time": "2022-03-05T08:59:32.605692Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.update({\"device\": device})\n",
    "print(config[\"device\"])\n",
    "net = net.to(device)\n",
    "net = nn.DataParallel(net)\n",
    "temporary_dictionary = train(\n",
    "    net,\n",
    "    train_iterator,\n",
    "    test_iterator,\n",
    "    loss_function,\n",
    "    config[\"number_epochs\"],\n",
    "    config[\"number_epochs_decay\"],\n",
    "    optimizer,\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"device\"],\n",
    "    model_save_path,\n",
    ")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:44:26.670311Z",
     "start_time": "2022-03-05T09:44:26.628701Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(xlsx_path):\n",
    "    pd.DataFrame.from_dict(temporary_dictionary,orient='columns').to_excel(xlsx_path, sheet_name=config[\"net_name\"] + \"_statistics\")\n",
    "else:\n",
    "    writer = pd.ExcelWriter(xlsx_path, mode=\"a\", engine=\"openpyxl\")\n",
    "    pd.DataFrame.from_dict(temporary_dictionary,orient='columns').to_excel(writer, sheet_name=config[\"net_name\"] + \"_statistics\")\n",
    "    writer.save()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
