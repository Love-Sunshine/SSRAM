{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T00:11:14.273492Z",
     "start_time": "2021-09-03T00:11:11.649336Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-16T08:26:46.977359Z",
     "iopub.status.busy": "2021-07-16T08:26:46.976760Z",
     "iopub.status.idle": "2021-07-16T08:26:49.706903Z",
     "shell.execute_reply": "2021-07-16T08:26:49.705463Z",
     "shell.execute_reply.started": "2021-07-16T08:26:46.977278Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "initial_directory = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchsummary\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "from network.PG_SN import PG_SN\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # 以下面设置的第一个卡为主卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,0\"  # 物理卡号\n",
    "\n",
    "config={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T00:11:14.552584Z",
     "start_time": "2021-09-03T00:11:14.276558Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config.update(\n",
    "    {\n",
    "        \"size\": 180,\n",
    "        \"in_channels\": 1,\n",
    "        \"encoder_channels\": [32, 64, 128, 256, 512],\n",
    "        \"decoder_channels\": [512, 256, 128, 64, 32],\n",
    "        \"out_channels\": 2,\n",
    "    }\n",
    ")\n",
    "config.update({\"net_name\": \"PG_SN\"})\n",
    "net = PG_SN(config)\n",
    "print(net(torch.randn(8, 1, 180, 180)).shape)\n",
    "config.update({\"parameters\": sum(param.numel() for param in net.parameters())})\n",
    "print(config[\"parameters\"])\n",
    "config.update({\"flops\": FlopCountAnalysis(net, (torch.randn(8, 1, 180, 180),)).total()})\n",
    "print(config[\"flops\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        assert data.shape[1] == 3, \"The data does not meet the requirements.\"\n",
    "        self.data = self.get_data(data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        origin, segmentation, label = self.data[index]\n",
    "        t=origin.reshape(origin.shape[0],-1)\n",
    "        t=(t-t.mean(axis=1,keepdim=True))/torch.max(t.std(axis=1,keepdim=True),1.0/torch.tensor(t.shape[1]*1.0).sqrt())\n",
    "        origin = t.reshape(origin.shape)\n",
    "        return origin.float(), segmentation.long(), int(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def get_data(self, data):\n",
    "        total = []\n",
    "        transformer = transforms.Compose([transforms.ToTensor()])\n",
    "        for i in tnrange(data.shape[0], dynamic_ncols=True, desc=\"get_data\"):\n",
    "            assert (os.path.exists(data[i][0]) and os.path.isfile((data[i][0])) and os.path.exists(data[i][1]) and os.path.isfile((data[i][1])))\n",
    "            origin = np.uint16(np.load(data[i][0]))\n",
    "            segmentation = np.uint16(np.load(data[i][1]))\n",
    "            assert (len(origin.shape) == 3 and len(segmentation.shape) == 3 and origin.shape == segmentation.shape)\n",
    "            for j in range(origin.shape[2]):\n",
    "                if len(np.unique(segmentation[:, :, j])) > 1:\n",
    "                    total.append([transformer(np.float32(origin[:,:,j])),transformer(np.float32(segmentation[:,:,j])/65535.0),int(data[i][2])])\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T00:11:14.871030Z",
     "start_time": "2021-09-03T00:11:14.816928Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluation(data_iterator, net, loss_function, device):\n",
    "    net.eval()\n",
    "    net = net.to(device)\n",
    "    loss, number = 0.0, 0\n",
    "    Acc, SE, SP, PC, F1, JS, DC = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    TP, TN, FP, FN = 0.0, 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, s, _ in tqdm(data_iterator, dynamic_ncols=True, leave=False, desc=\"test\"):\n",
    "            assert net.training == False\n",
    "            X = X.to(device)\n",
    "            s = s.to(device)\n",
    "            s_hat = net(X)\n",
    "            loss += loss_function(s_hat, s[:,0,:,:]).float().cpu().item() * s.shape[0]\n",
    "            s_hat = s_hat.detach().cpu().argmax(dim=1, keepdim=True).int().float()\n",
    "            TP += (((s_hat.int() == 1).int() + (s.cpu().int() == 1).int()) == 2).int().float().sum().item()\n",
    "            FP += (((s_hat.int() == 1).int() + (s.cpu().int() == 0).int()) == 2).int().float().sum().item()\n",
    "            FN += (((s_hat.int() == 0).int() + (s.cpu().int() == 1).int()) == 2).int().float().sum().item()\n",
    "            TN += (((s_hat.int() == 0).int() + (s.cpu().int() == 0).int()) == 2).int().float().sum().item()\n",
    "            number += s.shape[0]\n",
    "        Acc = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0.0# Accuracy\n",
    "        SE = TP / (TP + FN) if TP + FN > 0 else 0.0# Sensitivity == Recall\n",
    "        SP = TN / (TN + FP) if TN + FP > 0 else 0.0# Specificity\n",
    "        PC = TP / (TP + FP) if TP + FP > 0 else 0.0# Precision\n",
    "        F1 = 2 * TP / (2 * TP + FN + FP) if 2 * TP + FN + FP > 0 else 0.0# F1 == DC\n",
    "        JS = TP / (TP + FN + FP) if TP + FN + FP > 0 else 0.0# Jaccard Similarity\n",
    "        DC = 2 * TP / (2 * TP + FN + FP) if 2 * TP + FN + FP > 0 else 0.0# Dice Coefficient\n",
    "        print('[Validation] Loss: %.4f, Acc: %.4f, SE: %.4f, SP: %.4f, PC: %.4f, F1: %.4f, JS: %.4f, DC: %.4f' % (loss / number, Acc, SE, SP, PC, F1, JS, DC))\n",
    "    net.train()\n",
    "    return {'loss': loss / number,'Acc': Acc,'SE': SE,'SP': SP,'PC': PC,'F1': F1,'JS': JS,'DC': DC}\n",
    "\n",
    "\n",
    "def train(net,train_iterator,test_iterator,loss_function,number_epochs,number_epochs_decay,optimizer,learning_rate,device,model_save_path):\n",
    "    net.train()\n",
    "    net = net.to(device)\n",
    "    print(\"training on\", device)\n",
    "    temporary_dictionary = {\n",
    "        \"train_loss\": [],\"train_Acc\": [],\"train_SE\": [],\"train_SP\": [],\"train_PC\": [],\"train_F1\": [],\"train_JS\": [],\"train_DC\": [],\n",
    "        \"test_loss\": [],\"test_Acc\": [],\"test_SE\": [],\"test_SP\": [],\"test_PC\": [],\"test_F1\": [],\"test_JS\": [],\"test_DC\": [],\n",
    "    }\n",
    "    JS_score, DC_score = 0.0, 0.0\n",
    "    for epoch in tnrange(1, number_epochs + 1, dynamic_ncols=True, desc=\"epoch\"):\n",
    "        assert net.training == True\n",
    "        train_loss, number, start_time = 0.0, 0, time.time()\n",
    "        TP, TN, FP, FN = 0.0, 0.0, 0.0, 0.0\n",
    "        Acc, SE, SP, PC, F1, JS, DC = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for X, s, _ in tqdm(train_iterator, dynamic_ncols=True, leave=False, desc=\"train\"):\n",
    "            assert net.training == True\n",
    "            X = X.to(device)\n",
    "            s = s.to(device)\n",
    "            s_hat = net(X)\n",
    "            loss = loss_function(s_hat, s[:,0,:,:]).float()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.float().cpu().item() * s.shape[0]\n",
    "            s_hat = s_hat.detach().cpu().argmax(dim=1, keepdim=True).int().float()\n",
    "            TP += (((s_hat.int() == 1).int() + (s.cpu().int() == 1).int()) == 2).int().float().sum().item()\n",
    "            FP += (((s_hat.int() == 1).int() + (s.cpu().int() == 0).int()) == 2).int().float().sum().item()\n",
    "            FN += (((s_hat.int() == 0).int() + (s.cpu().int() == 1).int()) == 2).int().float().sum().item()\n",
    "            TN += (((s_hat.int() == 0).int() + (s.cpu().int() == 0).int()) == 2).int().float().sum().item()\n",
    "            number += s.shape[0]\n",
    "        Acc = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0.0#Accuracy\n",
    "        SE = TP / (TP + FN) if TP + FN > 0 else 0.0# Sensitivity == Recall\n",
    "        SP = TN / (TN + FP) if TN + FP > 0 else 0.0# Specificity\n",
    "        PC = TP / (TP + FP) if TP + FP > 0 else 0.0# Precision\n",
    "        F1 = 2 * TP / (2 * TP + FN + FP) if 2 * TP + FN + FP > 0 else 0.0# F1 == DC\n",
    "        JS = TP / (TP + FN + FP) if TP + FN + FP > 0 else 0.0# Jaccard Similarity\n",
    "        DC = 2 * TP / (2 * TP + FN + FP) if 2 * TP + FN + FP > 0 else 0.0# Dice Coefficient\n",
    "        print('Epoch [%d/%d]' % (epoch, number_epochs))\n",
    "        print('[Training] Loss: %.4f, Acc: %.4f, SE: %.4f, SP: %.4f, PC: %.4f, F1: %.4f, JS: %.4f, DC: %.4f' % (train_loss / number, Acc, SE, SP, PC, F1, JS, DC))\n",
    "        if epoch > (number_epochs - number_epochs_decay):\n",
    "            learning_rate *= (1 - epoch / number_epochs) ** 0.9\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "            print('Decay learning rate to lr: {}.'.format(learning_rate))\n",
    "        \n",
    "        test_dictionary = evaluation(test_iterator, net, loss_function, device)\n",
    "\n",
    "        temporary_dictionary[\"train_loss\"].append(train_loss / number)\n",
    "        temporary_dictionary[\"train_Acc\"].append(Acc)\n",
    "        temporary_dictionary[\"train_SE\"].append(SE)\n",
    "        temporary_dictionary[\"train_SP\"].append(SP)\n",
    "        temporary_dictionary[\"train_PC\"].append(PC)\n",
    "        temporary_dictionary[\"train_F1\"].append(F1)\n",
    "        temporary_dictionary[\"train_JS\"].append(JS)\n",
    "        temporary_dictionary[\"train_DC\"].append(DC)\n",
    "\n",
    "        temporary_dictionary[\"test_loss\"].append(test_dictionary['loss'])\n",
    "        temporary_dictionary[\"test_Acc\"].append(test_dictionary['Acc'])\n",
    "        temporary_dictionary[\"test_SE\"].append(test_dictionary['SE'])\n",
    "        temporary_dictionary[\"test_SP\"].append(test_dictionary['SP'])\n",
    "        temporary_dictionary[\"test_PC\"].append(test_dictionary['PC'])\n",
    "        temporary_dictionary[\"test_F1\"].append(test_dictionary['F1'])\n",
    "        temporary_dictionary[\"test_JS\"].append(test_dictionary['JS'])\n",
    "        temporary_dictionary[\"test_DC\"].append(test_dictionary['DC'])\n",
    "\n",
    "        if test_dictionary['JS'] > JS_score or test_dictionary['DC'] > DC_score:\n",
    "            JS_score, DC_score = test_dictionary['JS'], test_dictionary['DC']\n",
    "            print('Epoch %d : Best %s model JS score %.4f and DC score %.4f.' % (epoch, config[\"net_name\"], test_dictionary['JS'], test_dictionary['DC']))\n",
    "            torch.save(net, os.path.join(model_save_path, str(epoch) + \".pth\"))\n",
    "        print('Time %.1f sec' % (time.time() - start_time))\n",
    "    return temporary_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T00:11:35.443472Z",
     "start_time": "2021-09-03T00:11:14.873085Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_file_path = os.path.abspath(\"../data/data_192_save_as_resampled_qu/npy/npy_data_patients_xyz.xlsx\")\n",
    "config.update(\n",
    "    {\n",
    "        \"batch_size\": 64,\n",
    "        \"learning_rate\": 0.0002,\n",
    "        \"number_epochs\": 20,\n",
    "        \"number_epochs_decay\": 3,\n",
    "        \"test_size\": 0.1,\n",
    "    }\n",
    ")\n",
    "data = pd.read_excel(data_file_path)\n",
    "train_data, test_data = train_test_split(\n",
    "    data.values,\n",
    "    test_size=config[\"test_size\"],\n",
    "    random_state=42,\n",
    "    stratify=data.values[:, 2:],\n",
    ")\n",
    "train_dataset = My_Dataset(train_data)\n",
    "test_dataset = My_Dataset(test_data)\n",
    "train_iterator = torch.utils.data.DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "test_iterator = torch.utils.data.DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=config[\"learning_rate\"])\n",
    "model_save_path = os.path.abspath(\"../model/\" + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "pd.DataFrame(train_data, columns=data.columns).to_excel(os.path.join(model_save_path, 'train_data.xlsx'), sheet_name=\"train_data\", index=False)\n",
    "pd.DataFrame(test_data, columns=data.columns).to_excel(os.path.join(model_save_path, 'test_data.xlsx'), sheet_name=\"test_data\", index=False)\n",
    "pd.DataFrame.from_dict(config, orient='index').to_excel(os.path.join(model_save_path, 'config.xlsx'), sheet_name=\"config\")\n",
    "xlsx_path = os.path.join(model_save_path, config[\"net_name\"] + \".xlsx\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T01:51:26.779704Z",
     "start_time": "2021-09-03T00:11:35.445455Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.update({\"device\": device})\n",
    "print(config[\"device\"])\n",
    "net = net.to(device)\n",
    "net = nn.DataParallel(net)\n",
    "temporary_dictionary = train(\n",
    "    net,\n",
    "    train_iterator,\n",
    "    test_iterator,\n",
    "    loss_function,\n",
    "    config[\"number_epochs\"],\n",
    "    config[\"number_epochs_decay\"],\n",
    "    optimizer,\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"device\"],\n",
    "    model_save_path,\n",
    ")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T01:51:28.597480Z",
     "start_time": "2021-09-03T01:51:28.566922Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(xlsx_path):\n",
    "    pd.DataFrame.from_dict(temporary_dictionary,orient='columns').to_excel(xlsx_path, sheet_name=config[\"net_name\"] + \"_statistics\")\n",
    "else:\n",
    "    writer = pd.ExcelWriter(xlsx_path, mode=\"a\", engine=\"openpyxl\")\n",
    "    pd.DataFrame.from_dict(temporary_dictionary,orient='columns').to_excel(writer, sheet_name=config[\"net_name\"] + \"_statistics\")\n",
    "    writer.save()\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a2c0d98b13fda38b0681c30c6cd4339d800972ff59f70cc67da81e6bacc73e2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
